# -*- coding: utf-8 -*-
"""5_algorithms_analysis_of_india.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uCyZnackrTKnES0iapCM1HdH1ZozV7p7

covid 19 data use for 5 alogorithm
Comparing Linear Regression, Random Forest Regression,
 XGBoost, LSTMs, and ARIMA Time Series Forecasting
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from xgboost.sklearn import XGBRegressor

import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.layers import LSTM
from keras.utils import np_utils
from keras.optimizers import Adam 
from keras.callbacks import EarlyStopping
import statsmodels.api as sm
import statsmodels.tsa.api as smt
from sklearn.model_selection import KFold, cross_val_score, train_test_split

import pickle

df = pd.read_csv('/content/covid_19_clean_complete.csv',parse_dates=['Date'])
df.rename(columns={'ObservationDate':'Date', 'Country/Region':'Country'}, inplace=True)

df.head()

df_data = df.groupby(["Date", "Country", "Province/State"])[['Date', 'Province/State', 'Country', 'Confirmed', 'Deaths', 'Recovered']].sum().reset_index()
df_data.head()

"""## Corona cases in Italy """

df.query('Country=="Italy"').groupby("Date")[['Confirmed', 'Deaths', 'Recovered']].sum().reset_index()



df1=df.groupby('Date').sum()
df1.head()

confirmed = df.groupby('Date').sum()['Confirmed'].reset_index()
deaths = df.groupby('Date').sum()['Deaths'].reset_index()
recovered = df.groupby('Date').sum()['Recovered'].reset_index()

confirmed.head()

def monthly_cases(data):
    monthly_data = data.copy()
    monthly_data.Date = monthly_data.Date.apply(lambda x: str(x)[:-3])
    monthly_data = monthly_data.groupby('Date')['Confirmed'].sum().reset_index()
    monthly_data.Date = pd.to_datetime(monthly_data.Date)
    return monthly_data

monthly_df = monthly_cases(confirmed)
monthly_df

def confirmed_cases_per_day():
    fig, ax = plt.subplots(figsize=(7,4))
    plt.hist(monthly_df.Confirmed, color='mediumblue')
    
    ax.set(xlabel = "confirmed Per day",
           ylabel = "Cases",
           title = "Distrobution of confirmred cases Per Day")
    
confirmed_cases_per_day()

# Average monthly confirmed cases

# Overall
avg_monthly_cases = monthly_df.Confirmed.mean()
print(f"Overall average monthly confirmed cases: ${avg_monthly_cases}")

"""# Determining Stationarity"""

def time_plot(data, x_col, y_col, title):
    fig, ax = plt.subplots(figsize=(15,5))
    sns.lineplot(x_col, y_col, data=data, ax=ax, color='mediumblue', label='Total cases')
    
    second = data.groupby(data.Date.dt.year)[y_col].mean().reset_index()
    second.Date = pd.to_datetime(second.Date, format='%Y')
    sns.lineplot((second.Date + datetime.timedelta(365/12)), y_col, data=second, ax=ax, color='red', label='Mean cases')   
    
    ax.set(xlabel = "Date",
           ylabel = "confirmed",
           title = title)
    
    sns.despine()

time_plot(monthly_df, 'Date', 'Confirmed','Monthly confirmed cases Before Diff Transformation')

def get_diff(data):
    data['Confirmed_diff'] = data.Confirmed.diff()
    data = data.dropna()
    
    return data

stationary_df = get_diff(monthly_df)

time_plot(stationary_df, 'Date', 'Confirmed_diff', 'Monthly cases After Diff Transformation')

def plots(data, lags=None):
    
    # Convert dataframe to datetime index
    dt_data = data.set_index('Date').drop('Confirmed', axis=1)
    dt_data.dropna(axis=0)
    
    layout = (1, 3)
    raw  = plt.subplot2grid(layout, (0, 0))
    acf  = plt.subplot2grid(layout, (0, 1))
    pacf = plt.subplot2grid(layout, (0, 2))
    
    dt_data.plot(ax=raw, figsize=(12, 5), color='mediumblue')
    smt.graphics.plot_acf(dt_data, lags=lags, ax=acf, color='mediumblue')
    smt.graphics.plot_pacf(dt_data, lags=lags, ax=pacf, color='mediumblue')
    sns.despine()
    plt.tight_layout()

plots(stationary_df, lags=24);

"""# Preparing Dataset Modeling"""

#create dataframe for transformation from time series to supervised
def generate_supervised(data):
    supervised_df = data.copy()
    
    #create column for each lag
    for i in range(1,13):
        col_name = 'lag_' + str(i)
        supervised_df[col_name] = supervised_df['Confirmed_diff'].shift(i)
    
    #drop null values
    supervised_df = supervised_df.dropna().reset_index(drop=True)
    
   # supervised_df.to_csv('../data/model_df.csv', index=False)
    
    return supervised_df

model_df = generate_supervised(stationary_df)
model_df



"""# ARIMA Modeling"""

def generate_arima_data(data):
    dt_data = data.set_index('Date').drop('Confirmed', axis=1)
    dt_data.dropna(axis=0)
    
    #dt_data.to_csv('../data/arima_df.csv')
    
    return dt_data

datetime_df = generate_arima_data(stationary_df)
datetime_df

"""# Train Test Split"""

def tts(data):
    data = data.drop(['Date','Confirmed'],axis=1)
    train, test = data[0:-12].values, data[-12:].values
    
    return train, test

train, test = tts(model_df)

train.shape

test.shape

def scale_data(train_set, test_set):
    #apply Min Max Scaler
    scaler = MinMaxScaler(feature_range=(-1, 1))
    scaler = scaler.fit(train_set)
    
    # reshape training set
    train_set = train_set.reshape(train_set.shape[0], train_set.shape[1])
    train_set_scaled = scaler.transform(train_set)
    
    # reshape test set
    test_set = test_set.reshape(test_set.shape[0], test_set.shape[1])
    test_set_scaled = scaler.transform(test_set)
    
    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1].ravel()
    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1].ravel()
    
    return X_train, y_train, X_test, y_test, scaler

X_train, y_train, X_test, y_test, scaler_object = scale_data(train, test)

y_train.shape

"""# Modeling Functions"""

def undo_scaling(y_pred, x_test, scaler_obj, lstm=False):  
    #reshape y_pred
    y_pred = y_pred.reshape(y_pred.shape[0], 1, 1)
    
    if not lstm:
        x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])
    
    #rebuild test set for inverse transform
    pred_test_set = []
    for index in range(0,len(y_pred)):
        pred_test_set.append(np.concatenate([y_pred[index],x_test[index]],axis=1))
        
    #reshape pred_test_set
    pred_test_set = np.array(pred_test_set)
    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0], pred_test_set.shape[2])
    
    #inverse transform
    pred_test_set_inverted = scaler_obj.inverse_transform(pred_test_set)
    
    return pred_test_set_inverted

def load_original_df():
    #load in original dataframe without scaling applied
    original_df = confirmed
    original_df.Date = original_df.Date.apply(lambda x: str(x)[:-3])
    original_df = original_df.groupby('Date')['Confirmed'].sum().reset_index()
    original_df.Date = pd.to_datetime(original_df.Date)
    #df['Timestamp'] = pd.to_datetime(df['Year'],format='%Y') 
    return original_df

def predict_df(unscaled_predictions, original_df):
    #create dataframe that shows the predicted cases
    result_list = []
    cases_dates = list(original_df[-14:].Date)
    act_confirmed = list(original_df[-14:].Confirmed)

    for index in range(0,len(unscaled_predictions)):
        result_dict = {}
        result_dict['pred_value'] = int(unscaled_predictions[index][0] + act_confirmed[index])
        result_dict['Date'] = cases_dates[index+1]
        result_list.append(result_dict)
        
    df_result = pd.DataFrame(result_list)
        
    return df_result

model_scores = {}

def get_scores(unscaled_df, original_df, model_name):
    rmse = np.sqrt(mean_squared_error(original_df.Confirmed[-12:], unscaled_df.pred_value[-12:]))
    mae = mean_absolute_error(original_df.Confirmed[-12:], unscaled_df.pred_value[-12:])
    r2 = r2_score(original_df.Confirmed[-12:], unscaled_df.pred_value[-12:])
    model_scores[model_name] = [rmse, mae, r2]

    print(f"RMSE: {rmse}")
    print(f"MAE: {mae}")
    print(f"R2 Score: {r2}")

def plot_results(results, original_df, model_name):

    fig, ax = plt.subplots(figsize=(15,5))
    sns.lineplot(original_df.Date, original_df.Confirmed, data=original_df, ax=ax,label='Original', color='mediumblue')
    sns.lineplot(results.Date, results.pred_value, data=results, ax=ax,label='Predicted', color='Red')
    
    ax.set(xlabel = "Date",
           ylabel = "Confirmed",
           title = f"{model_name} corona Forecasting Prediction")
    
    ax.legend()
    
    sns.despine()
    
#plt.savefig(f'../model_output/{model_name}_forecast.png')

def run_model(train_data, test_data, model, model_name):
    
    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data, test_data)
    
    mod = model
    mod.fit(X_train, y_train)
    predictions = mod.predict(X_test)
    
    # Undo scaling to compare predictions against original data
    original_df = load_original_df()
    unscaled = undo_scaling(predictions, X_test, scaler_object)
    unscaled_df = predict_df(unscaled, original_df)
      
    get_scores(unscaled_df, original_df, model_name)
    
    plot_results(unscaled_df, original_df, model_name)

"""# Linear Regression"""

run_model(train, test, LinearRegression(), 'LinearRegression')

"""# Random Forest Regressor"""

run_model(train, test, RandomForestRegressor(n_estimators=100, max_depth=20),'RandomForest')

"""# XGBoost"""

run_model(train, test, XGBRegressor( n_estimators=100, learning_rate=0.2, objective='reg:squarederror'), 'XGBoost')

"""# LSTM"""

def lstm_model(train_data, test_data):
    
    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data, test_data)
    
    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])
   
    model = Sequential()
    model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), 
                   stateful=True))
    model.add(Dense(1))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(X_train, y_train, epochs=200, batch_size=1, verbose=1, 
              shuffle=False)
    predictions = model.predict(X_test,batch_size=1)
    
    original_df = confirmed
    unscaled = undo_scaling(predictions, X_test, scaler_object, lstm=True)
    unscaled_df = predict_df(unscaled, original_df)
    
    get_scores(unscaled_df, original_df, 'LSTM')
    
    plot_results(unscaled_df, original_df, 'LSTM')

lstm_model(train, test)

def main():
    """Calls all functions to load data, run regression models, run lstm model,
    and run arima model.
    """
    # Regression models
    model_df = load_data('../data/model_df.csv')
    train, test = tts(model_df)

    # Sklearn
    regressive_model(train, test, LinearRegression(), 'LinearRegression')
    regressive_model(train, test, RandomForestRegressor(n_estimators=100,
                                                        max_depth=20),
                     'RandomForest')
    regressive_model(train, test, XGBRegressor(n_estimators=100,
                                               learning_rate=0.2,
                                               objective='reg:squarederror'),
                     'XGBoost')
    # Keras
    lstm_model(train, test)

    

main()

pickle.dump(model_scores, open( "model_scores.p", "wb" ) )


